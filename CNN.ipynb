{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-11-30T13:37:58.454860Z",
     "start_time": "2024-11-30T13:37:58.432901Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "462add2f08efe715",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:38:01.017566Z",
     "start_time": "2024-11-30T13:38:01.008117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Set up the device (CUDA if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: Custom Dataset Class to Load Images and Labels\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image using OpenCV\n",
    "        img = cv2.imread(self.image_paths[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "        \n",
    "        # Convert the NumPy array to a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        # Apply the transformations (resize, normalize, etc.)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Get the label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)  # Ensure label is torch.long\n",
    "        \n",
    "        return img, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:38:01.873043Z",
     "start_time": "2024-11-30T13:38:01.856137Z"
    }
   },
   "id": "d3a252d2fb0e45cc",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1658 images from 9 classes:\n",
      "Class 'Ajwa' contains the following images:\n",
      "  - 175 images\n",
      "Class 'Galaxy' contains the following images:\n",
      "  - 190 images\n",
      "Class 'Medjool' contains the following images:\n",
      "  - 135 images\n",
      "Class 'Meneifi' contains the following images:\n",
      "  - 232 images\n",
      "Class 'Nabtat Ali' contains the following images:\n",
      "  - 177 images\n",
      "Class 'Rutab' contains the following images:\n",
      "  - 146 images\n",
      "Class 'Shaishe' contains the following images:\n",
      "  - 171 images\n",
      "Class 'Sokari' contains the following images:\n",
      "  - 264 images\n",
      "Class 'Sugaey' contains the following images:\n",
      "  - 168 images\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load the 1658 Colored Images \n",
    "def load_colored_images(dataset_path):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Ensure that class folders are correctly identified\n",
    "    class_names = sorted([folder for folder in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, folder))])  # Class folders (9 classes)\n",
    "    \n",
    "    # Loop through each class folder\n",
    "    for class_index, class_name in enumerate(class_names):\n",
    "        class_folder = os.path.join(dataset_path, class_name)\n",
    "        \n",
    "        # Loop through each image in the class folder\n",
    "        for img_name in os.listdir(class_folder):\n",
    "            img_path = os.path.join(class_folder, img_name)\n",
    "            if img_name.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):  # Check for valid image extensions\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(class_index)  # Assign the class label\n",
    "    \n",
    "    return image_paths, np.array(labels), class_names\n",
    "\n",
    "# Path to your dataset (colored images)\n",
    "dataset_path = 'D:\\\\Desktop\\\\AI361\\\\project\\\\clean_resized'\n",
    "\n",
    "# Load the image paths and labels for the 1658 colored images\n",
    "image_paths, labels, class_names = load_colored_images(dataset_path)\n",
    "\n",
    "# Output some useful information to verify the function worked\n",
    "print(f\"Loaded {len(image_paths)} images from {len(class_names)} classes:\")\n",
    "for class_name in class_names:\n",
    "    print(f\"Class '{class_name}' contains the following images:\")\n",
    "    class_image_paths = [image_paths[i] for i in range(len(labels)) if labels[i] == class_names.index(class_name)]\n",
    "    print(f\"  - {len(class_image_paths)} images\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:38:04.144478Z",
     "start_time": "2024-11-30T13:38:04.121559Z"
    }
   },
   "id": "36fba025eaf0a1d6",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 5: Data Augmentation and Normalization for Colored Images\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Random crop and resize to 224x224\n",
    "    transforms.RandomHorizontalFlip(),   # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(),     # Random vertical flip\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Random color changes\n",
    "    transforms.RandomRotation(30),       # Random rotation between -30 and +30 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  # Random affine transformation\n",
    "    transforms.RandomGrayscale(p=0.2),   # Randomly convert 20% of images to grayscale\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5, interpolation=3),  # Random perspective transformation\n",
    "    transforms.GaussianBlur(kernel_size=3),  # Apply Gaussian blur\n",
    "    transforms.ToTensor(),  # Convert image to tensor and normalize to [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for pre-trained models\n",
    "])\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = CustomImageDataset(image_paths, labels, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:38:09.165300Z",
     "start_time": "2024-11-30T13:38:09.154344Z"
    }
   },
   "id": "bb13607678c1d605",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 600 noisy grayscale images.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Load 600 Noisy Grayscale Images\n",
    "def load_noisy_grayscale_images(noisy_images_path):\n",
    "    noisy_images = []\n",
    "    image_names = []  # To store image names\n",
    "    \n",
    "    for img_name in os.listdir(noisy_images_path):\n",
    "        if img_name.endswith('.jpg') or img_name.endswith('.png'):\n",
    "            img_path = os.path.join(noisy_images_path, img_name)\n",
    "            \n",
    "            # Read image in grayscale\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (224, 224))  # Resize image to 224x224\n",
    "            \n",
    "            # Normalize image to [0, 1]\n",
    "            img = img.astype('float32') / 255.0\n",
    "            \n",
    "            # Convert grayscale to 3 channels (RGB) by repeating the single channel\n",
    "            img = np.expand_dims(img, axis=-1)  # Shape: (224, 224, 1)\n",
    "            img = np.repeat(img, 3, axis=-1)    # Shape: (224, 224, 3)\n",
    "            \n",
    "            # Convert to Tensor and normalize\n",
    "            img = transforms.ToTensor()(img)\n",
    "            img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "            \n",
    "            noisy_images.append(img)\n",
    "            image_names.append(img_name)  # Store the name of the image\n",
    "    \n",
    "    return torch.stack(noisy_images), image_names\n",
    "\n",
    "# Path to the folder containing the 600 noisy grayscale images\n",
    "noisy_images_path = 'D:\\\\Desktop\\\\AI361\\\\project\\\\enhanced-images31'\n",
    "\n",
    "# Load noisy grayscale images\n",
    "noisy_grayscale_images, noisy_image_names = load_noisy_grayscale_images(noisy_images_path)\n",
    "print(f\"Loaded {len(noisy_grayscale_images)} noisy grayscale images.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:38:14.187459Z",
     "start_time": "2024-11-30T13:38:11.524405Z"
    }
   },
   "id": "1b1cb9d73c5da021",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModifiedCNNModel(\n",
      "  (mobilenet_v2): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=62720, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc5): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc6): Linear(in_features=16, out_features=9, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Define CNN Model for Classification\n",
    "class ModifiedCNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedCNNModel, self).__init__()\n",
    "        \n",
    "        # Use pre-trained MobileNetV2 as the backbone\n",
    "        self.mobilenet_v2 = models.mobilenet_v2(pretrained=True).features\n",
    "        \n",
    "        # Freeze the layers of MobileNetV2\n",
    "        for param in self.mobilenet_v2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Custom fully connected layers after the MobileNetV2 feature extractor\n",
    "        # Update the input size of fc1 to 62720 (1280 * 7 * 7)\n",
    "        self.fc1 = nn.Linear(1280 * 7 * 7, 256)  # MobileNetV2 output channels = 1280, spatial size = 7x7\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.fc6 = nn.Linear(16, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass input through MobileNetV2 feature extractor\n",
    "        x = self.mobilenet_v2(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 1280 * 7 * 7)\n",
    "        \n",
    "        # Fully connected layers with ReLU activations and Dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final output layer\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 9  # Replace with the number of output classes\n",
    "model = ModifiedCNNModel(num_classes=num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:38:16.468782Z",
     "start_time": "2024-11-30T13:38:15.831791Z"
    }
   },
   "id": "ab04cb3d85837a4",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 2.2155, Accuracy: 13.69%\n",
      "Accuracy: 0.0690, Precision: 0.1034, Recall: 0.0690, AUC: 0.5373\n",
      "Epoch 2/300, Loss: 2.1851, Accuracy: 15.62%\n",
      "Accuracy: 0.2586, Precision: 0.3090, Recall: 0.2586, AUC: 0.5919\n",
      "Epoch 3/300, Loss: 2.1118, Accuracy: 19.90%\n",
      "Accuracy: 0.3103, Precision: 0.2102, Recall: 0.3103, AUC: 0.6476\n",
      "Epoch 4/300, Loss: 2.0220, Accuracy: 23.28%\n",
      "Accuracy: 0.1724, Precision: 0.2891, Recall: 0.1724, AUC: 0.6948\n",
      "Epoch 5/300, Loss: 1.9592, Accuracy: 24.85%\n",
      "Accuracy: 0.2931, Precision: 0.2603, Recall: 0.2931, AUC: 0.6705\n",
      "Epoch 6/300, Loss: 1.8937, Accuracy: 29.25%\n",
      "Accuracy: 0.2414, Precision: 0.1915, Recall: 0.2414, AUC: 0.6134\n",
      "Epoch 7/300, Loss: 1.8489, Accuracy: 30.40%\n",
      "Accuracy: 0.3103, Precision: 0.2780, Recall: 0.3103, AUC: 0.7210\n",
      "Epoch 8/300, Loss: 1.8251, Accuracy: 30.76%\n",
      "Accuracy: 0.4310, Precision: 0.4935, Recall: 0.4310, AUC: 0.7855\n",
      "Error in AUC calculation: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Error in AUC calculation: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Epoch 9/300, Loss: 1.8489, Accuracy: 30.64%\n",
      "Accuracy: 0.3103, Precision: 0.2684, Recall: 0.3103, AUC: nan\n",
      "Error in AUC calculation: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Epoch 10/300, Loss: 1.7748, Accuracy: 32.21%\n",
      "Accuracy: 0.3276, Precision: 0.3266, Recall: 0.3276, AUC: 0.7551\n",
      "Error in AUC calculation: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Epoch 11/300, Loss: 1.7431, Accuracy: 32.39%\n",
      "Accuracy: 0.2931, Precision: 0.2426, Recall: 0.2931, AUC: 0.6914\n",
      "Error in AUC calculation: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "Epoch 12/300, Loss: 1.7224, Accuracy: 32.75%\n",
      "Accuracy: 0.3448, Precision: 0.3284, Recall: 0.3448, AUC: 0.7612\n",
      "Epoch 13/300, Loss: 1.7347, Accuracy: 34.68%\n",
      "Accuracy: 0.3103, Precision: 0.2191, Recall: 0.3103, AUC: 0.7721\n",
      "Epoch 14/300, Loss: 1.7009, Accuracy: 34.38%\n",
      "Accuracy: 0.2586, Precision: 0.2530, Recall: 0.2586, AUC: 0.7179\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Set up Loss Function and Optimizer\n",
    "# Define the model (assuming a custom model is defined like in your previous code)\n",
    "# Initialize the model\n",
    "num_classes = 9  # Replace with the number of output classes\n",
    "model = ModifiedCNNModel(num_classes=num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer (same as in code 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Callback for early stopping (this is a manual implementation in PyTorch)\n",
    "early_stopping_patience = 30\n",
    "best_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Metrics to track (similar to the TensorFlow/Keras code)\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def compute_metrics(outputs, labels, num_classes):\n",
    "    # Apply softmax to get class probabilities\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Get the predicted class from probabilities\n",
    "    predicted = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "    \n",
    "    # Calculate precision and recall (weighted average)\n",
    "    precision = precision_score(labels.cpu(), predicted.cpu(), average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels.cpu(), predicted.cpu(), average='weighted', zero_division=0)\n",
    "    \n",
    "    # Calculate AUC for multi-class\n",
    "    auc = float('nan')  # Initialize with NaN, as default\n",
    "    if len(torch.unique(labels)) > 1:  # Only compute AUC if there are at least 2 unique classes in the batch\n",
    "        try:\n",
    "            # One-hot encode labels and calculate AUC\n",
    "            one_hot_labels = torch.eye(num_classes)[labels.cpu()].numpy()\n",
    "            auc = roc_auc_score(one_hot_labels, probs.cpu().detach().numpy(), multi_class='ovr', average='weighted')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in AUC calculation: {e}\")\n",
    "            auc = float('nan')  # If AUC fails, set it to NaN\n",
    "    \n",
    "    return accuracy, precision, recall, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # For each batch in the train_loader\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move inputs and labels to GPU if available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy, precision, recall, auc = compute_metrics(outputs, labels, num_classes)\n",
    "        \n",
    "        correct_predictions += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "    \n",
    "    # Print training statistics for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Early stopping logic (manual)\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # Save model checkpoint if needed\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-30T13:38:18.916430Z"
    }
   },
   "id": "78d394e86ff626e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Ajwa\n",
      "Images: 117.jpg, 213.jpg, 433.jpg, 442.jpg, 445.jpg, 460.jpg, 47.jpg, 508.jpg, 510.jpg, 525.jpg, 585.jpg\n",
      "\n",
      "Class: Galaxy\n",
      "Images: \n",
      "\n",
      "Class: Medjool\n",
      "Images: 1.jpg, 10.jpg, 100.jpg, 101.jpg, 103.jpg, 107.jpg, 108.jpg, 109.jpg, 11.jpg, 111.jpg, 112.jpg, 113.jpg, 114.jpg, 116.jpg, 118.jpg, 12.jpg, 120.jpg, 121.jpg, 122.jpg, 123.jpg, 125.jpg, 126.jpg, 127.jpg, 128.jpg, 129.jpg, 13.jpg, 130.jpg, 131.jpg, 132.jpg, 133.jpg, 134.jpg, 135.jpg, 136.jpg, 137.jpg, 138.jpg, 139.jpg, 141.jpg, 142.jpg, 143.jpg, 144.jpg, 145.jpg, 146.jpg, 147.jpg, 148.jpg, 149.jpg, 15.jpg, 150.jpg, 151.jpg, 152.jpg, 153.jpg, 154.jpg, 155.jpg, 156.jpg, 157.jpg, 158.jpg, 159.jpg, 16.jpg, 160.jpg, 161.jpg, 162.jpg, 163.jpg, 164.jpg, 165.jpg, 166.jpg, 167.jpg, 168.jpg, 169.jpg, 17.jpg, 170.jpg, 171.jpg, 173.jpg, 174.jpg, 175.jpg, 176.jpg, 177.jpg, 178.jpg, 179.jpg, 18.jpg, 180.jpg, 181.jpg, 183.jpg, 184.jpg, 185.jpg, 186.jpg, 187.jpg, 188.jpg, 189.jpg, 19.jpg, 190.jpg, 191.jpg, 192.jpg, 195.jpg, 197.jpg, 198.jpg, 199.jpg, 2.jpg, 20.jpg, 201.jpg, 202.jpg, 203.jpg, 204.jpg, 205.jpg, 207.jpg, 208.jpg, 209.jpg, 21.jpg, 210.jpg, 212.jpg, 214.jpg, 215.jpg, 216.jpg, 217.jpg, 218.jpg, 22.jpg, 220.jpg, 221.jpg, 222.jpg, 224.jpg, 225.jpg, 226.jpg, 227.jpg, 228.jpg, 229.jpg, 23.jpg, 230.jpg, 232.jpg, 233.jpg, 234.jpg, 235.jpg, 238.jpg, 239.jpg, 24.jpg, 240.jpg, 241.jpg, 242.jpg, 244.jpg, 245.jpg, 246.jpg, 247.jpg, 248.jpg, 25.jpg, 251.jpg, 252.jpg, 253.jpg, 255.jpg, 256.jpg, 257.jpg, 258.jpg, 26.jpg, 260.jpg, 261.jpg, 263.jpg, 264.jpg, 27.jpg, 271.jpg, 273.jpg, 274.jpg, 275.jpg, 277.jpg, 278.jpg, 279.jpg, 28.jpg, 282.jpg, 283.jpg, 286.jpg, 287.jpg, 288.jpg, 289.jpg, 29.jpg, 290.jpg, 291.jpg, 293.jpg, 296.jpg, 297.jpg, 298.jpg, 299.jpg, 3.jpg, 30.jpg, 301.jpg, 302.jpg, 303.jpg, 304.jpg, 305.jpg, 307.jpg, 309.jpg, 31.jpg, 310.jpg, 311.jpg, 312.jpg, 313.jpg, 314.jpg, 315.jpg, 316.jpg, 317.jpg, 318.jpg, 319.jpg, 32.jpg, 320.jpg, 321.jpg, 322.jpg, 323.jpg, 324.jpg, 325.jpg, 326.jpg, 327.jpg, 328.jpg, 33.jpg, 330.jpg, 332.jpg, 333.jpg, 334.jpg, 335.jpg, 337.jpg, 338.jpg, 34.jpg, 340.jpg, 342.jpg, 343.jpg, 344.jpg, 347.jpg, 349.jpg, 35.jpg, 350.jpg, 351.jpg, 352.jpg, 353.jpg, 354.jpg, 356.jpg, 357.jpg, 359.jpg, 360.jpg, 361.jpg, 363.jpg, 364.jpg, 365.jpg, 366.jpg, 367.jpg, 369.jpg, 37.jpg, 371.jpg, 372.jpg, 373.jpg, 374.jpg, 376.jpg, 377.jpg, 378.jpg, 379.jpg, 382.jpg, 383.jpg, 385.jpg, 387.jpg, 388.jpg, 389.jpg, 39.jpg, 390.jpg, 391.jpg, 392.jpg, 393.jpg, 394.jpg, 396.jpg, 397.jpg, 398.jpg, 4.jpg, 40.jpg, 400.jpg, 402.jpg, 403.jpg, 404.jpg, 405.jpg, 406.jpg, 409.jpg, 41.jpg, 411.jpg, 412.jpg, 413.jpg, 414.jpg, 415.jpg, 416.jpg, 417.jpg, 418.jpg, 419.jpg, 42.jpg, 420.jpg, 421.jpg, 422.jpg, 423.jpg, 424.jpg, 425.jpg, 426.jpg, 427.jpg, 428.jpg, 429.jpg, 43.jpg, 431.jpg, 434.jpg, 435.jpg, 437.jpg, 438.jpg, 439.jpg, 44.jpg, 441.jpg, 443.jpg, 444.jpg, 446.jpg, 447.jpg, 448.jpg, 449.jpg, 45.jpg, 450.jpg, 451.jpg, 454.jpg, 455.jpg, 456.jpg, 457.jpg, 458.jpg, 46.jpg, 461.jpg, 463.jpg, 464.jpg, 465.jpg, 467.jpg, 468.jpg, 470.jpg, 471.jpg, 472.jpg, 474.jpg, 476.jpg, 477.jpg, 478.jpg, 479.jpg, 480.jpg, 481.jpg, 482.jpg, 483.jpg, 484.jpg, 485.jpg, 486.jpg, 487.jpg, 488.jpg, 489.jpg, 491.jpg, 492.jpg, 493.jpg, 495.jpg, 497.jpg, 499.jpg, 5.jpg, 50.jpg, 500.jpg, 501.jpg, 502.jpg, 503.jpg, 506.jpg, 507.jpg, 509.jpg, 51.jpg, 511.jpg, 513.jpg, 514.jpg, 515.jpg, 516.jpg, 517.jpg, 518.jpg, 519.jpg, 52.jpg, 520.jpg, 521.jpg, 522.jpg, 523.jpg, 524.jpg, 526.jpg, 527.jpg, 528.jpg, 529.jpg, 531.jpg, 532.jpg, 534.jpg, 535.jpg, 536.jpg, 538.jpg, 539.jpg, 54.jpg, 540.jpg, 541.jpg, 542.jpg, 543.jpg, 544.jpg, 545.jpg, 546.jpg, 547.jpg, 548.jpg, 549.jpg, 55.jpg, 552.jpg, 553.jpg, 554.jpg, 555.jpg, 556.jpg, 559.jpg, 560.jpg, 561.jpg, 562.jpg, 563.jpg, 564.jpg, 565.jpg, 567.jpg, 568.jpg, 569.jpg, 57.jpg, 570.jpg, 571.jpg, 573.jpg, 574.jpg, 575.jpg, 579.jpg, 58.jpg, 580.jpg, 581.jpg, 582.jpg, 583.jpg, 584.jpg, 587.jpg, 588.jpg, 590.jpg, 592.jpg, 593.jpg, 594.jpg, 595.jpg, 596.jpg, 597.jpg, 6.jpg, 60.jpg, 600.jpg, 62.jpg, 63.jpg, 64.jpg, 65.jpg, 66.jpg, 67.jpg, 68.jpg, 7.jpg, 70.jpg, 71.jpg, 72.jpg, 74.jpg, 75.jpg, 76.jpg, 77.jpg, 79.jpg, 8.jpg, 80.jpg, 81.jpg, 82.jpg, 84.jpg, 85.jpg, 86.jpg, 87.jpg, 89.jpg, 9.jpg, 91.jpg, 92.jpg, 94.jpg, 96.jpg, 98.jpg\n",
      "\n",
      "Class: Meneifi\n",
      "Images: \n",
      "\n",
      "Class: Nabtat Ali\n",
      "Images: \n",
      "\n",
      "Class: Rutab\n",
      "Images: 105.jpg, 110.jpg, 115.jpg, 172.jpg, 193.jpg, 211.jpg, 231.jpg, 254.jpg, 262.jpg, 267.jpg, 276.jpg, 292.jpg, 294.jpg, 336.jpg, 339.jpg, 345.jpg, 348.jpg, 36.jpg, 370.jpg, 384.jpg, 386.jpg, 401.jpg, 452.jpg, 453.jpg, 469.jpg, 473.jpg, 475.jpg, 49.jpg, 498.jpg, 505.jpg, 53.jpg, 533.jpg, 537.jpg, 557.jpg, 558.jpg, 566.jpg, 572.jpg, 586.jpg, 598.jpg, 599.jpg, 73.jpg, 83.jpg, 88.jpg, 90.jpg, 97.jpg, 99.jpg\n",
      "\n",
      "Class: Shaishe\n",
      "Images: \n",
      "\n",
      "Class: Sokari\n",
      "Images: 102.jpg, 104.jpg, 106.jpg, 119.jpg, 14.jpg, 140.jpg, 182.jpg, 194.jpg, 196.jpg, 200.jpg, 206.jpg, 219.jpg, 223.jpg, 236.jpg, 237.jpg, 243.jpg, 249.jpg, 250.jpg, 259.jpg, 265.jpg, 266.jpg, 268.jpg, 270.jpg, 272.jpg, 280.jpg, 281.jpg, 284.jpg, 285.jpg, 295.jpg, 300.jpg, 306.jpg, 308.jpg, 331.jpg, 341.jpg, 346.jpg, 358.jpg, 362.jpg, 368.jpg, 375.jpg, 380.jpg, 381.jpg, 395.jpg, 399.jpg, 407.jpg, 408.jpg, 410.jpg, 430.jpg, 432.jpg, 436.jpg, 440.jpg, 459.jpg, 462.jpg, 466.jpg, 48.jpg, 490.jpg, 494.jpg, 496.jpg, 504.jpg, 512.jpg, 530.jpg, 550.jpg, 551.jpg, 56.jpg, 576.jpg, 577.jpg, 578.jpg, 589.jpg, 59.jpg, 591.jpg, 61.jpg, 69.jpg, 78.jpg, 95.jpg\n",
      "\n",
      "Class: Sugaey\n",
      "Images: 124.jpg, 269.jpg, 329.jpg, 355.jpg, 38.jpg, 93.jpg\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Classify Noisy Grayscale Images with the Trained Model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "noisy_grayscale_images = noisy_grayscale_images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(noisy_grayscale_images)\n",
    "    _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "# Convert predicted labels to numpy array\n",
    "predicted_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "# Create a dictionary to store the images belonging to each class\n",
    "class_image_map = {class_name: [] for class_name in class_names}\n",
    "\n",
    "# Map the images to their predicted classes\n",
    "for idx, label in enumerate(predicted_labels):\n",
    "    class_image_map[class_names[label]].append(noisy_image_names[idx])\n",
    "\n",
    "# Display the results\n",
    "for class_name, images in class_image_map.items():\n",
    "    print(f\"Class: {class_name}\")\n",
    "    print(f\"Images: {', '.join(images)}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-30T13:11:37.595362Z",
     "start_time": "2024-11-30T13:11:32.223630Z"
    }
   },
   "id": "33286610a02410c7",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8b763dd4f2c77435"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_env)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
